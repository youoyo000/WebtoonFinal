from flask import Flask, render_template, Response, stream_with_context, jsonify, request
import requests
from bs4 import BeautifulSoup
import re
import time
import json
import os
from flask_cors import CORS 

# åˆå§‹åŒ– Flask
app = Flask(__name__)
CORS(app)

# ==========================================
# ğŸ”´ æ ¸å¿ƒä¿®æ­£ï¼šä½¿ç”¨çµ•å°è·¯å¾‘ï¼Œç¢ºä¿ä¸€å®šæ‰¾å¾—åˆ°æª”æ¡ˆ
# ==========================================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_FILE = os.path.join(BASE_DIR, 'comics_data.json')

# å•Ÿå‹•æ™‚å°å‡ºè·¯å¾‘ï¼Œæ–¹ä¾¿é™¤éŒ¯
print("="*50)
print(f"ğŸ“‚ ç³»çµ±å•Ÿå‹•ä¸­...")
print(f"ğŸ“‚ è³‡æ–™åº«è·¯å¾‘å·²é–å®šç‚º: {DATA_FILE}")
print("="*50)

# --- å·¥å…·å‡½å¼ ---

def get_title_no(hyperlink):
    """å¾ç¶²å€è§£æå”¯ä¸€çš„ title_no"""
    match = re.search(r"title_no=(\d+)", hyperlink)
    if match: return match.group(1)
    match2 = re.search(r'/list\?title_no=(\d+)', hyperlink)
    return match2.group(1) if match2 else None

def get_episode_count_by_html(html_content):
    """è§£æ HTML ç²å–æœ€æ–°è©±æ¬¡è™Ÿç¢¼ (æ•¸å­—)"""
    try:
        soup = BeautifulSoup(html_content, "html.parser")
        episode_list = soup.find("ul", id="_listUl")
        if not episode_list: return 0
        latest_item = episode_list.find("li", class_="_episodeItem")
        if latest_item and "data-episode-no" in latest_item.attrs:
            return int(latest_item["data-episode-no"])
        return 0
    except Exception:
        return 0

def load_local_data():
    """è®€å–æœ¬åœ° JSON ä¸¦è½‰ç‚º {id: data} çš„å­—å…¸æ ¼å¼ä»¥ä¾¿å¿«é€Ÿæ¯”å°"""
    if not os.path.exists(DATA_FILE):
        return {}
    try:
        with open(DATA_FILE, 'r', encoding='utf-8') as f:
            content = f.read().strip()
            if not content: return {} # é˜²æ­¢ç©ºæª”æ¡ˆå ±éŒ¯
            data_list = json.loads(content)
            # å°‡ List è½‰ç‚º Dictionaryï¼ŒKey æ˜¯ comic_id
            return {item['id']: item for item in data_list}
    except Exception as e:
        print(f"è®€å– JSON å¤±æ•—: {e}")
        return {}

def save_local_data(data_dict):
    """å°‡å­—å…¸è½‰å› List ä¸¦å­˜å…¥ JSON"""
    try:
        data_list = list(data_dict.values())
        with open(DATA_FILE, 'w', encoding='utf-8') as f:
            json.dump(data_list, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"å­˜æª”å¤±æ•—: {e}")

# --- è·¯ç”±è¨­å®š ---

@app.route('/')
def home():
    return "Webtoon Crawler API is Running!"

@app.route('/api/proxy-image')
def proxy_image():
    url = request.args.get('url')
    if not url: return "No URL", 400
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Referer": "https://www.webtoons.com/"
        }
        resp = requests.get(url, headers=headers, timeout=15)
        excluded_headers = ['content-encoding', 'content-length', 'transfer-encoding', 'connection']
        headers = [(name, value) for (name, value) in resp.raw.headers.items() if name.lower() not in excluded_headers]
        return Response(resp.content, resp.status_code, headers)
    except Exception as e:
        return str(e), 500

@app.route('/api/comics')
def get_comics_api():
    # åŠ å…¥è·¯å¾‘æª¢æŸ¥ï¼Œè®“ä½ åœ¨çµ‚ç«¯æ©Ÿçœ‹åˆ°å®ƒæœ‰æ²’æœ‰æ‰¾åˆ°
    if not os.path.exists(DATA_FILE):
        print(f"âŒ API è«‹æ±‚å¤±æ•—ï¼šæ‰¾ä¸åˆ°æª”æ¡ˆæ–¼ {DATA_FILE}")
        return jsonify([])
    
    try:
        with open(DATA_FILE, 'r', encoding='utf-8') as f:
            content = f.read().strip()
            if not content: return jsonify([])
            return jsonify(json.loads(content))
    except Exception as e:
        print(f"âŒ è®€å–éŒ¯èª¤: {e}")
        return jsonify([])

@app.route('/start-crawl')
def start_crawl():
    def generate():
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36",
            "Referer": "https://www.webtoons.com/"
        }
        
        yield "data: ğŸš€ çˆ¬èŸ²å•Ÿå‹•ï¼šæ¯”å°æœ¬åœ° JSON æ¨¡å¼\n\n"
        
        # 1. è¼‰å…¥æœ¬åœ°è³‡æ–™åº«
        local_db = load_local_data()
        yield f"data: ğŸ“‚ ç›®å‰æœ¬åœ°è³‡æ–™åº«å…±æœ‰ {len(local_db)} éƒ¨æ¼«ç•«\n\n"

        # 2. å–å¾—ç¸½é æ•¸ (ä»¥å®Œçµå€ç‚ºä¾‹)
        first_url = "https://www.webtoons.com/zh-hant/originals/complete?sortOrder=UPDATE&page=1"
        try:
            res = requests.get(first_url, headers=headers)
            soup = BeautifulSoup(res.text, "html.parser")
            max_page = 1
            for a in soup.select('div.paginate > a'):
                try:
                    p = int(a.text.strip())
                    if p > max_page: max_page = p
                except: continue
            yield f"data: ğŸ“¦ ç·šä¸Šæ¸…å–®å…± {max_page} é ï¼Œé–‹å§‹æƒæ...\n\n"
        except Exception as e:
            yield f"data: âŒ ç„¡æ³•é€£æ¥ Webtoon: {str(e)}\n\n"
            return

        total_updated = 0
        total_new = 0
        total_skipped = 0
        
        # 3. é–‹å§‹åˆ†é çˆ¬å–
        for page in range(1, max_page + 1): 
            url = f"https://www.webtoons.com/zh-hant/originals/complete?sortOrder=UPDATE&page={page}"
            yield f"data: ğŸ“„ æ­£åœ¨æƒæç¬¬ {page} / {max_page} é ...\n\n"
            
            try:
                res = requests.get(url, headers=headers)
                soup = BeautifulSoup(res.text, "html.parser")
                comics = soup.select('a.link._originals_title_a')
            except Exception as e:
                yield f"data: âŒ è®€å–é é¢å¤±æ•—: {str(e)}\n\n"
                continue
            
            page_dirty = False 

            for comic_a in comics:
                try:
                    # --- å–å¾—åˆ—è¡¨é è³‡è¨Š ---
                    title = comic_a.select_one('.title').text.strip()
                    hyperlink = comic_a['href']
                    genre = comic_a.select_one('.genre').text.strip()
                    title_no = get_title_no(hyperlink)
                    
                    if not title_no: continue
                    comic_id = title_no

                    # --- é—œéµï¼šæª¢æŸ¥æ˜¯å¦éœ€è¦æ›´æ–° ---
                    # å¿…é ˆå…ˆæŠ“å–å…§é æ‰èƒ½çŸ¥é“è©±æ¬¡æœ‰æ²’æœ‰è®Šå¤šï¼Œé€™æ˜¯å¿…è¦çš„ Request
                    res_detail = requests.get(hyperlink, headers=headers)
                    res_detail.encoding = "utf-8"
                    
                    # å–å¾—ç›®å‰ç·šä¸Šæœ€æ–°è©±æ¬¡ (æ•´æ•¸)
                    current_episode_count = get_episode_count_by_html(res_detail.text)
                    current_episodes_str = f"å…± {current_episode_count} è©±"

                    # æ¯”å°é‚è¼¯
                    old_data = local_db.get(comic_id)
                    is_new = False
                    is_update = False

                    if old_data is None:
                        is_new = True
                        yield f"data: âœ… ç™¼ç¾æ–°æ¼«ç•«ï¼š{title}\n\n"
                    else:
                        # æ¯”å°è©±æ¬¡æ•¸é‡ (ä½¿ç”¨ .get é¿å…èˆŠè³‡æ–™æ²’æœ‰è©²æ¬„ä½å ±éŒ¯)
                        old_count = old_data.get('episode_count', 0)
                        
                        if current_episode_count > old_count:
                            is_update = True
                            yield f"data: ğŸ”„ ç™¼ç¾æ›´æ–°ï¼š{title} ({old_count} -> {current_episode_count})\n\n"
                        else:
                            # è³‡æ–™å®Œå…¨ä¸€æ¨£ï¼Œç›´æ¥ç•¥é
                            # yield f"data: â­ï¸ ç•¥éï¼š{title} (ç„¡è®Šæ›´)\n\n" 
                            total_skipped += 1
                            time.sleep(0.05) # ç¨å¾®ä¼‘æ¯æ¥µçŸ­æ™‚é–“
                            continue 

                    # --- å¦‚æœæ˜¯æ–°è³‡æ–™æˆ–æ›´æ–°ï¼Œæ‰åŸ·è¡Œè§£æèˆ‡å„²å­˜ ---
                    soup2 = BeautifulSoup(res_detail.text, "html.parser")
                    
                    cover_tag = soup2.select_one(".detail_header .thmb img") or soup2.select_one("img")
                    picture = cover_tag["src"] if cover_tag else ""
                    
                    author_tag = soup2.select_one(".author")
                    author = author_tag.get_text(strip=True) if author_tag else "æœªçŸ¥"
                    
                    access_note = "å·²å®Œçµï¼Œå¯å…è²»çœ‹å®Œæ•´è©±æ•¸!"
                    if soup2.find(string=lambda t: t and "åœ¨APPå¯ä»¥é–±è®€æ›´å¤šè©±æ¬¡" in t):
                        access_note = "å·²å®Œçµï¼Œéœ€è¦è¿½æ¼«åˆ¸"

                    current_time = time.strftime("%Y-%m-%d %H:%M:%S")

                    # å»ºç«‹è³‡æ–™ç‰©ä»¶
                    doc = {
                        "id": comic_id,
                        "title": title,
                        "genre": genre,
                        "author": author,
                        "episodes": current_episodes_str,
                        "episode_count": current_episode_count, # å­˜å…¥æ•¸å­—æ–¹ä¾¿ä¸‹æ¬¡æ¯”å°
                        "access": access_note,
                        "picture": picture,
                        "hyperlink": hyperlink,
                        "last_updated": current_time,
                        "crawl_date": current_time
                    }

                    # å¦‚æœæ˜¯æ›´æ–°ï¼Œä¿ç•™åŸæœ¬çš„ crawl_date (åˆæ¬¡çˆ¬å–æ™‚é–“)
                    if is_update and old_data:
                        doc['crawl_date'] = old_data.get('crawl_date', current_time)

                    # å¯«å…¥è¨˜æ†¶é«”ä¸­çš„å­—å…¸
                    local_db[comic_id] = doc
                    page_dirty = True 

                    if is_new: total_new += 1
                    if is_update: total_updated += 1
                    
                    time.sleep(0.1) # æœ‰çˆ¬å–å‹•ä½œæ™‚ï¼Œä¼‘æ¯ä¹…ä¸€é»é¿å…è¢«æ“‹

                except Exception as inner_e:
                    yield f"data: âŒ è™•ç† {title} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(inner_e)}\n\n"
            
            # è©²é é¢å…¨éƒ¨è·‘å®Œå¾Œï¼Œå¦‚æœæœ‰è®Šå‹•æ‰å¯«å…¥ç¡¬ç¢Ÿ
            if page_dirty:
                save_local_data(local_db)
                yield f"data: ğŸ’¾ ç¬¬ {page} é è³‡æ–™å·²æ›´æ–°ä¸¦å­˜æª”\n\n"
            
            yield f"data: ğŸ ç¬¬ {page} é å®Œæˆ\n\n"

        yield f"data: ğŸ‰ ä»»å‹™çµæŸï¼æ–°å¢: {total_new}ï¼Œæ›´æ–°: {total_updated}ï¼Œç•¥é: {total_skipped}ã€‚\n\n"
        yield "data: DONE\n\n"

    return Response(stream_with_context(generate()), mimetype='text/event-stream')

if __name__ == "__main__":
    # ğŸ”´ ç¢ºä¿ Port æ˜¯ 5000ï¼Œæ‰èƒ½å°æ‡‰åˆ° React çš„è¨­å®š
    app.run(debug=True, port=5000)