from flask import Flask, render_template, Response, stream_with_context, jsonify
import requests
from bs4 import BeautifulSoup
import re
import time
import json
import os

# åˆå§‹åŒ– Flask
app = Flask(__name__)

# --- è¨­å®š JSON è³‡æ–™åº«è·¯å¾‘ ---
DATA_FILE = 'comics_data.json'

# --- å·¥å…·å‡½å¼ ---

def get_title_no(hyperlink):
    """å¾ç¶²å€æå–å”¯ä¸€ ID (title_no)"""
    match = re.search(r"title_no=(\d+)", hyperlink)
    if match: return match.group(1)
    match2 = re.search(r'/list\?title_no=(\d+)', hyperlink)
    return match2.group(1) if match2 else None

def get_episode_count_by_html(html_content):
    """ç›´æ¥å¾è©³ç´°é  HTML è§£æ data-episode-no"""
    try:
        soup = BeautifulSoup(html_content, "html.parser")
        episode_list = soup.find("ul", id="_listUl")
        if not episode_list: return 0
        latest_item = episode_list.find("li", class_="_episodeItem")
        if latest_item and "data-episode-no" in latest_item.attrs:
            return int(latest_item["data-episode-no"])
        return 0
    except Exception:
        return 0

def load_local_data():
    """è®€å–æœ¬åœ° JSON æª”æ¡ˆï¼Œè½‰æˆ Dictionary ä»¥ä¾¿å¿«é€Ÿæ¯”å°"""
    if not os.path.exists(DATA_FILE):
        return {}
    try:
        with open(DATA_FILE, 'r', encoding='utf-8') as f:
            data_list = json.load(f)
            # è½‰æˆ Dict æ ¼å¼: { "comic_id": {è³‡æ–™...}, ... }
            return {item['id']: item for item in data_list}
    except Exception as e:
        print(f"è®€å– JSON å¤±æ•—: {e}")
        return {}

def save_local_data(data_dict):
    """å°‡ Dictionary è½‰å› List ä¸¦å­˜å…¥ JSON"""
    try:
        data_list = list(data_dict.values())
        with open(DATA_FILE, 'w', encoding='utf-8') as f:
            json.dump(data_list, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"å­˜æª”å¤±æ•—: {e}")

# --- è·¯ç”±è¨­å®š ---

@app.route('/')
def home():
    return render_template('dashboard.html') # è«‹ç¢ºä¿ä½ æœ‰ dashboard.html

# æ–°å¢é€™å€‹ API è®“å‰ç«¯ Dashboard æŠ“è³‡æ–™
@app.route('/api/comics')
def get_comics_api():
    if not os.path.exists(DATA_FILE):
        return jsonify([])
    with open(DATA_FILE, 'r', encoding='utf-8') as f:
        return jsonify(json.load(f))

@app.route('/start-crawl')
def start_crawl():
    """åŸ·è¡Œçˆ¬èŸ²ä¸¦å³æ™‚å›å‚³é€²åº¦ (JSON ç‰ˆ)"""
    def generate():
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36",
            "Referer": "https://www.webtoons.com/"
        }
        
        yield "data: ğŸš€ çˆ¬èŸ²ç³»çµ±å•Ÿå‹• (æœ¬åœ° JSON æ¨¡å¼)...\n\n"
        
        # 1. å…ˆæŠŠèˆŠè³‡æ–™å…¨éƒ¨è®€é€²ä¾† (è¨˜æ†¶é«”å¿«å–)
        local_db = load_local_data()
        yield f"data: ğŸ“‚ å·²è¼‰å…¥æœ¬åœ°è³‡æ–™åº«ï¼Œå…± {len(local_db)} ç­†è³‡æ–™\n\n"

        # 2. å–å¾—ç¸½é æ•¸ (é€™æ®µä¿æŒä¸è®Š)
        first_url = "https://www.webtoons.com/zh-hant/originals/complete?sortOrder=UPDATE&page=1"
        try:
            res = requests.get(first_url, headers=headers)
            soup = BeautifulSoup(res.text, "html.parser")
            max_page = 1
            for a in soup.select('div.paginate > a'):
                try:
                    p = int(a.text.strip())
                    if p > max_page: max_page = p
                except: continue
            yield f"data: ğŸ“¦ åµæ¸¬åˆ°å®Œçµæ¼«ç•«å…± {max_page} é \n\n"
        except Exception as e:
            yield f"data: âŒ åˆå§‹é€£ç·šå¤±æ•—: {str(e)}\n\n"
            return

        total_processed = 0
        total_updated = 0
        total_skipped = 0
        
        # æ­£å¼ç’°å¢ƒè«‹ç”¨ range(1, max_page + 1)
        for page in range(1, 3): 
            url = f"https://www.webtoons.com/zh-hant/originals/complete?sortOrder=UPDATE&page={page}"
            yield f"data: ğŸ“„ æ­£åœ¨è®€å–ç¬¬ {page} é æ¸…å–®...\n\n"
            
            try:
                res = requests.get(url, headers=headers)
                soup = BeautifulSoup(res.text, "html.parser")
                comics = soup.select('a.link._originals_title_a')
            except Exception as e:
                yield f"data: âŒ è®€å–é é¢å¤±æ•—: {str(e)}\n\n"
                continue
            
            # ç”¨æ–¼æ¨™è¨˜æ˜¯å¦éœ€è¦å­˜æª” (æ¯ä¸€é å­˜ä¸€æ¬¡ï¼Œé¿å…å¤ªé »ç¹å¯«ç¡¬ç¢Ÿ)
            page_dirty = False 

            for comic_a in comics:
                try:
                    title = comic_a.select_one('.title').text.strip()
                    hyperlink = comic_a['href']
                    genre = comic_a.select_one('.genre').text.strip()
                    title_no = get_title_no(hyperlink)
                    comic_id = title_no if title_no else str(int(time.time()))

                    yield f"data: ğŸ” åˆ†æä¸­ï¼š{title}...\n\n"

                    # è«‹æ±‚è©³ç´°é 
                    res_detail = requests.get(hyperlink, headers=headers)
                    res_detail.encoding = "utf-8"
                    
                    episode_count = get_episode_count_by_html(res_detail.text)
                    current_episodes_str = f"å…± {episode_count} è©±"

                    # === ã€é—œéµä¿®æ”¹ã€‘ç›´æ¥å¾è¨˜æ†¶é«” (local_db) æ¯”å°ï¼Œä¸é€£ç·šè³‡æ–™åº« ===
                    old_data = local_db.get(comic_id)
                    
                    if old_data and old_data.get('episodes') == current_episodes_str:
                        yield f"data: â­ï¸ è©±æ•¸ç„¡è®Šæ›´ ({episode_count})ï¼Œè·³éæ›´æ–°\n\n"
                        total_skipped += 1
                        time.sleep(0.05) 
                        continue 
                    
                    # --- éœ€è¦æ›´æ–° ---
                    soup2 = BeautifulSoup(res_detail.text, "html.parser")
                    cover_tag = soup2.select_one(".detail_header .thmb img") or soup2.select_one("img")
                    picture = cover_tag["src"] if cover_tag else ""
                    
                    author_tag = soup2.select_one(".author")
                    author = author_tag.get_text(strip=True) if author_tag else "æœªçŸ¥"
                    
                    access_note = "å·²å®Œçµï¼Œå¯å…è²»çœ‹å®Œæ•´è©±æ•¸!"
                    if soup2.find(string=lambda t: t and "åœ¨APPå¯ä»¥é–±è®€æ›´å¤šè©±æ¬¡" in t):
                        access_note = "å·²å®Œçµï¼Œéœ€è¦è¿½æ¼«åˆ¸"

                    current_time = time.strftime("%Y-%m-%d %H:%M:%S")

                    # å»ºç«‹æ–°è³‡æ–™ç‰©ä»¶
                    doc = {
                        "id": comic_id, # JSON éœ€è¦æŠŠ ID å¯«åœ¨è£¡é¢
                        "title": title,
                        "genre": genre,
                        "author": author,
                        "episodes": current_episodes_str,
                        "episode_count": episode_count, # å­˜æ•¸å­—æ–¹ä¾¿æœªä¾†æ’åº
                        "access": access_note,
                        "picture": picture,
                        "hyperlink": hyperlink,
                        "crawl_date": current_time,
                        "last_updated": current_time
                    }

                    # æ›´æ–°è¨˜æ†¶é«”ä¸­çš„è³‡æ–™
                    local_db[comic_id] = doc
                    page_dirty = True # æ¨™è¨˜è³‡æ–™å·²è®Šå‹•

                    if old_data:
                        yield f"data: ğŸ”„ æ›´æ–°è³‡æ–™ï¼š{title}\n\n"
                    else:
                        yield f"data: âœ… æ–°å¢è³‡æ–™ï¼š{title}\n\n"
                        
                    total_updated += 1
                    total_processed += 1
                    time.sleep(0.1)

                except Exception as inner_e:
                    yield f"data: âŒ éŒ¯èª¤: {str(inner_e)}\n\n"
            
            # --- æ¯ä¸€é è™•ç†å®Œå¾Œï¼Œå¦‚æœæœ‰æ›´æ–°ï¼Œå°±å­˜æª”ä¸€æ¬¡ ---
            if page_dirty:
                save_local_data(local_db)
                yield f"data: ğŸ’¾ ç¬¬ {page} é è³‡æ–™å·²å­˜æª”\n\n"
            
            yield f"data: ğŸ ç¬¬ {page} é å®Œæˆ\n\n"

        yield f"data: ğŸ‰ ä»»å‹™çµæŸï¼æ›´æ–°: {total_updated}ï¼Œç•¥é: {total_skipped}ã€‚\n\n"
        yield "data: DONE\n\n"

    return Response(stream_with_context(generate()), mimetype='text/event-stream')

if __name__ == "__main__":
    app.run(debug=True, port=5001)